from __future__ import annotations

import pprint
import random
from dataclasses import dataclass
from typing import List
from typing import Tuple

import matplotlib.pyplot as plt
import numpy as np
import pyspark as ps
from pyspark.sql import SparkSession
from sklearn.metrics.pairwise import manhattan_distances
from sklearn_extra.cluster import KMedoids

from classeDePissio import ClasseDePissio

# Create SparkSession
spark = SparkSession.builder \
    .master("local[*]") \
    .appName("PAMAE") \
    .getOrCreate()

sc = spark.sparkContext
ds_import: ps.RDD[np.ndarray[float]] = sc.textFile("datasets/google_review_ratings_2columns_allrows.csv").map(
    lambda line: line.split(",")).map(
    lambda x: to_float_conversion(x))


def to_float_conversion(line: List[str]) -> np.ndarray[float]:
    float_lst = np.float_(line)
    return float_lst


def distributed_sampling_and_global_search(
        dataset: ps.RDD[np.ndarray[float]],
        n_bins: int,
        sample_size: int,
        t: int
) -> list[dict]:
    """
    Random samples are generated by calling the "get_random_samples" function, and in them all possible groups of
    medoids are calculated (according to the "global search" logic) and then the best ones are chosen for each samples.
    Finally, we use the best medoids set for calculating again the t clusters, but this time on the entire dataset.
    After have partitioned the data we update each cluster only with the points already inside of them,
    this type of update is called "local search"

    :param dataset: original dataset
    :param n_bins: number of parts in which to divide the dataset
    :param sample_size: first n elements from each def_bin/partition of the original dataset
    :param t: desired number of clusters
    :return:
    """

    samples: ps.RDD[Sample] = get_random_samples(ds_import, n_bins, sample_size)
    print(samples.collect())
    best_medoids = global_search(samples, t, sample_size)
    refinement(best_medoids, dataset, t)


@dataclass
class Sample:
    key: int
    rows: List[np.ndarray[float]]





def get_random_samples(dataset: ps.RDD[np.ndarray[float]], m: int, n: int) -> ps.RDD[Sample]:
    """
    Random samples are generated by dividing the entire data set into m parts and taking the first n elements of each
    :param dataset: original dataset
    :param m: number of parts in which to divide the dataset
    :param n: first n elements from each def_bin/partition of the original dataset
    :return: RDD that contains all samples, grouped according to the key generated by the "random_mod" function.
    """

    # TODO Genere chiave rnd, calcolo il modulo e lo assegno alle tuple
    # TODO verificare se "Tuple[int, np.ndarray[float]]" si possa riorganizzare come un oggetto tipo "dato" visto che è
    # TODO sempre uguale

    def random_mod(row: np.ndarray[float]) -> Tuple[int, np.ndarray[float]]:
        # m = numero di campioni desiderati
        rnd = random.randrange(0, 9)
        mod = rnd % m
        return mod, row

    ds_with_mod: ps.RDD[Tuple[int, np.ndarray[float]]] = dataset.map(lambda row: random_mod(row))
    print(sorted(ds_with_mod.groupByKey().mapValues(len).collect()))

    # List è un "interfaccia" che mi permette di generalizzare i tipi di lista suggeriti in input
    # https://docs.python.org/3/library/typing.html
    ds_grouped: ps.RDD[Tuple[int, List[np.ndarray[float]]]] = sc.parallelize(
        ds_with_mod.groupByKey().mapValues(list).collect())

    def create_sample(row: Tuple[int, List[np.ndarray[float]]]) -> Sample:
        return Sample(row[0], row[1][:n])

    ds_samples = ds_grouped.map(create_sample)
    #ds_samples = ds_grouped.map(lambda row: Sample(row[0], row[1][:n]))
    print("DS_SAMPLES: ", type(ds_samples))
    # Aggiungi questa riga di codice per stampare la struttura di row
    return ds_samples


def global_search(samples: ps.RDD[Sample], t: int, sample_size: int) -> np.ndarray:
    """
    Phase I except for the sampling part

    :param samples: collection of objects Sample
    :param t: number of cluster (medoids)
    :param sample_size: sample size
    :return: 1D array of k elements containing the best medoids among the optimal medoids of each sample
    """
    rdd_global_search = samples.map(lambda sample: clustering(sample.rows, t, None, sample.key))
    # inizializza l'array degli errori
    errors = np.empty([0, 3])

    # salvo i risultati
    result = rdd_global_search.collect()

    # stampo i risultati
    for key, value in result:
        print(f"Campione {key}:")
        for i, cluster in enumerate(value['clusters']):
            print(f"Cluster {i}:")
            for point in cluster:
                print(point)
        print(f"Medoidi:")
        for medoid in value['medoids']:
            print(medoid)

        # aggiungi l'errore e i medoidi all'array degli errori
        errors = np.append(errors, np.array([medoid[0], medoid[1], value['error']]).reshape(1, -1), axis=0)
        print(f"Errore di clustering: {value['error']}")
        print()

    # ordina gli errori in ordine crescente di valore
    errori_ord = errors[errors[:, 2].argsort()]

    # stampa il set di medoidi con l'errore minimo
    print(f"Set di medoidi migliori: {errori_ord[0, 0:2]}")
    print(f"Errore minimo: {errori_ord[0, 2]}")

    print("---------------------------------------------------------")

    # calcola gli errori relativi
    normalized_errors = []

    # TODO: perchè normalized error stampa un solo valore?
    error_rel = errori_ord[0, 2] / sample_size
    normalized_errors.append(error_rel)

    print("Normalized Error: ", normalized_errors)
    best_medoids = errori_ord[0, 0:2]

    # TODO: commentare bene i comandi della libreria
    # plot dei risultati
    # plot dei risultati
    for key, value in result:
        plt.figure()
        for i, cluster in enumerate(value['clusters']):
            cluster = np.array(cluster)
            plt.scatter(cluster[:, 0], cluster[:, 1], label=f"Cluster {i}")
            medoid = np.array(value['medoids'][i])
            plt.scatter(medoid[0], medoid[1], marker='x', s=200, linewidths=3, color='r')
        plt.legend()
        plt.title(f"Campione {key}")
        plt.show()

    """
    PLOT IN 3 DIMENSIONI
    from mpl_toolkits.mplot3d import Axes3D
    
    fig = plt.figure()
    ax = fig.add_subplot(111, projection='3d')

    for i, cluster in enumerate(value['clusters']):
        cluster = np.array(cluster)
        ax.scatter(cluster[:, 0], cluster[:, 1], cluster[:, 2], label=f"Cluster {i}")
        medoid = np.array(value['clusters'][i])
        ax.scatter(medoid[0], medoid[1], medoid[2], marker='x', s=200, linewidths=3, color='r')

    ax.set_xlabel('X Label')
    ax.set_ylabel('Y Label')
    ax.set_zlabel('Z Label')
    plt.legend()
    plt.show()
    """
    print("BEST MEDOIDS: ", type(best_medoids))
    return best_medoids


def refinement(best_medoids: np.ndarray, dataset: ps.RDD, t: int) -> list[dict]:
    """
    Phase 2 of the algorithm presented in the PAMAE paper
    :param best_medoids: collection of the (ids of the) best medoids found in phase 1
    :param dataset: full dataset
    :param t: number of clusters
    :return: list of dictionaries where each dictionary represents a single iteration of clustering.
            Each dictionary has three keys:
            - medoids: a list of medoids, where each medoid is represented by a list of values;
            - clusters: a list of clusters, where each cluster is represented by a list of points,
                        where each point is represented by a list of values;
            - error: a numerical value representing the clustering error calculated for the current iteration.
    """

    # serializzo il collect di dataset per unire tutti gli array in una sola riga
    rdd_array = sc.parallelize(dataset.collect())

    # Otteniamo una lista di array dal RDD
    array_list = rdd_array.collect()

    # Creiamo una nuova RDD a partire dalla lista di array
    rdd_tuple = sc.parallelize([(array_list,)])

    # eseguo in maniera distribuita il clustering, row[0] è l'insieme dei punti del dataset intero
    rdd_refinement = rdd_tuple.map(lambda row: clustering(row[0], t, best_medoids))
    result = rdd_refinement.collect()

    print("RESULT: ")
    print(type(result))
    pp = pprint.PrettyPrinter(indent=2)
    pp.pprint(result)

    # calcolo l'errore relativo (ponderato in base al numero di punti)
    dataset_size = dataset.count()
    total_error = result[-1]['error']
    normalized_error = (total_error / dataset_size)
    print("Normalized Error: ", normalized_error)

    # plot dei risultati
    for value in result:
        plt.figure()
        for i, cluster in enumerate(value['clusters']):
            cluster = np.array(cluster)
            plt.scatter(cluster[:, 0], cluster[:, 1], label=f"Cluster {i}")
            medoid = np.array(value['medoids'][i])
            plt.scatter(medoid[0], medoid[1], marker='x', s=200, linewidths=3, color='r')
        plt.legend()
        plt.title("Clusters on the full dataset")
        plt.show()


    import pymongo
    import bson

    # Connessione al database MongoDB
    client = pymongo.MongoClient("mongodb://localhost:27017/")

    # Selezione del database e della collezione
    db = client["pamae"]
    collection = db["clustering_ouput"]

    # Codificare i dati in BSON
    encoded_data = [bson.BSON.encode(d) for d in result]

    # Inserire i dati nella collezione "results"
    collection.insert_many(encoded_data)


def clustering(sample: list, t: int, best_medoids=None, key: int = None):
    """

    :param sample: values from samples
    :param t: number of clusters
    :param best_medoids: set of best medoids from the  phase 1 (optional argument)
    :param key: the key that identify each samples (optional argument)
    :return:
    """

    def distances(values: list) -> np.ndarray:
        """
            >>> all_distances([[1, 2], [3, 4]])
            >>> np.ndarray([\
                [0., 4.],\
                [4., 0.]\
                ])

            :param samples: set of objects (dataset rows) sampled from the full dataset
            :return: 2D matrix where element ij is the distance between object (row) i and object (row) j
            """
        data = np.array(values)
        return manhattan_distances(data, data)

    # Calcolo la matrice di distanza
    distance_matrix = distances(sample)

    # Creo l'istanza del modello KMedoids
    if best_medoids is None:
        kmedoids_ = KMedoids(n_clusters=t, metric='precomputed', method="pam")
    else:
        # ClasseDePissio è una classe che eredita KMedoids e sovrascrive una parte di essa per generare i cluster con i
        # medoidi desiderati
        kmedoids_ = ClasseDePissio(n_clusters=t, init='random', metric='precomputed', method="pam",
                                   best_medoids=best_medoids)

    # Eseguo il clustering
    kmedoids_.fit(distance_matrix)

    # Recupero i medoidi
    medoids_idx = kmedoids_.medoid_indices_
    medoids = [sample[idx] for idx in medoids_idx]

    # Calcolo l'errore di clustering e assegno ad ogni punto il cluster di appartenenza
    labels = kmedoids_.labels_
    error = 0
    for i in range(len(sample)):
        error += distance_matrix[i, medoids_idx[labels[i]]]

    # Recupero i punti appartenenti ai cluster
    clusters = [[] for _ in range(t)]
    for i, label in enumerate(labels):
        clusters[label].append(sample[i])

    """
    nelle fase di global_search ho una key che identifica ogni sample, mentre nel refinement (lavorando sul dataset
    intero), non ho questa chiave. quindi disinguo due tipi di return, uno con la chiave per distingure i medoidi
    per campione, e l'altro (che verrà usato per il refinement) no.
    """
    if key is None:
        return {'medoids': medoids, 'clusters': clusters, 'error': error}
    else:
        return (key, {'medoids': medoids, 'clusters': clusters, 'error': error})


# launching the algorithm
distributed_sampling_and_global_search(ds_import, 2, 1100, 4)
