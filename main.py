import pprint
import random
from dataclasses import dataclass
from typing import List
from typing import Tuple

import matplotlib.pyplot as plt
import numpy as np
import pyspark as ps
from pyspark.sql import SparkSession
from sklearn.metrics.pairwise import manhattan_distances
from sklearn_extra.cluster import KMedoids

from classeDePissio import ClasseDePissio

import json
import pymongo

# Create SparkSession
spark = SparkSession.builder \
    .master("local[*]") \
    .appName("PAMAE") \
    .getOrCreate()

sc = spark.sparkContext
ds_import: ps.RDD[np.ndarray[float]] = sc.textFile("datasets/s1.csv").map(
    lambda line: line.split(",")).map(
    lambda x: to_float_conversion(x))


def to_float_conversion(line: List[str]) -> np.ndarray[float]:
    float_lst = np.float_(line)
    return float_lst


def pamae(
        dataset: ps.RDD[np.ndarray[float]],
        n_bins: int,
        sample_size: int,
        t: int
) -> list[dict]:
    """
    Random samples are generated by calling the "get_random_samples" function, and in them all possible groups of
    medoids are calculated (according to the "global search" logic) and then the best ones are chosen for each samples.
    Finally, we use the best medoids set for calculating again the t clusters, but this time on the entire dataset.
    After have partitioned the data we update each cluster only with the points already inside of them,
    this type of update is called "local search"

    :param dataset: original dataset
    :param n_bins: number of parts in which to divide the dataset
    :param sample_size: first n elements from each def_bin/partition of the original dataset
    :param t: desired number of clusters
    :return:
    """

    samples: ps.RDD[Sample] = get_random_samples(ds_import, n_bins, sample_size)
    print(samples.collect())
    best_medoids = parallel_seeding(samples, t, sample_size)
    clustering_result = parallel_refinement(best_medoids, dataset, t)
    export_to_mongo(clustering_result)


@dataclass
class Sample:
    key: int
    rows: List[np.ndarray[float]]


def get_random_samples(dataset: ps.RDD[np.ndarray[float]], m: int, n: int) -> ps.RDD[Sample]:
    """
    Random samples are generated by dividing the entire data set into m parts and taking the first n elements of each
    :param dataset: original dataset
    :param m: number of parts in which to divide the dataset
    :param n: first n elements from each partition of the original dataset
    :return: RDD that contains all samples, grouped according to the key generated by the "random_mod" function.
    """

    # Define a function to add a "mod" value to each row of the dataset
    def random_mod(row: np.ndarray[float]) -> Tuple[int, np.ndarray[float]]:
        # Choose a random integer between 0 and 9
        rnd = random.randrange(0, 9)
        # Compute mod of the random integer by m, to get the value for the "mod" key
        mod = rnd % m
        # Return a tuple with the "mod" value as the key and the row as the value
        return mod, row

    # Apply the "random_mod" function to each row of the dataset, resulting in an RDD of tuples with the "mod" value as key
    ds_with_mod: ps.RDD[Tuple[int, np.ndarray[float]]] = dataset.map(lambda row: random_mod(row))

    # Todo: perchÃ¨ len
    # Count the number of rows in each "mod" group and sort by key (mod value)
    print(sorted(ds_with_mod.groupByKey().mapValues(len).collect()))

    # Group the rows by "mod" value and convert the resulting RDD to an RDD of tuples with "mod" as key and a list of rows as value
    ds_grouped: ps.RDD[Tuple[int, List[np.ndarray[float]]]] = sc.parallelize(
        ds_with_mod.groupByKey().mapValues(list).collect())

    # Create a Sample object for each "mod" group, containing the "mod" value as id and the first n rows in the list as data
    def create_sample(row: Tuple[int, List[np.ndarray[float]]]) -> Sample:
        return Sample(row[0], row[1][:n])

    # Apply the "create_sample" function to each tuple in the RDD, resulting in an RDD of Sample objects
    ds_samples = ds_grouped.map(create_sample)

    # Print the type of the resulting RDD
    print("DS_SAMPLES: ", type(ds_samples))

    # Return the RDD of Sample objects
    return ds_samples


#TODO: CONTROLLARE OUTPUT
def clustering(sample: list, t: int, best_medoids=None, key: int = None) -> dict or tuple:
    """
    Perform clustering using K-medoids algorithm.

    :param sample: A list of objects (dataset rows) to be clustered.
    :param t: An integer representing the number of desired clusters.
    :param best_medoids: A set of best medoids from the phase 1 (optional argument).
    :param key: An integer representing the key that identifies each sample (optional argument).
    :return: A dictionary with the keys 'medoids', 'clusters', and 'error' if `key` is None, otherwise a tuple of the
            form (key, {medoids, clusters, error}).
    """

    def distances(values: list) -> np.ndarray:
        """
        Compute the pairwise Manhattan distance between a list of objects.

        :param values: A list of objects to compute pairwise Manhattan distance.
        :return: A 2D matrix where element ij is the distance between object (row) i and object (row) j.
        """
        data = np.array(values)
        return manhattan_distances(data, data)

    # Calculate the distance matrix
    distance_matrix = distances(sample)

    # Create an instance of the KMedoids model
    if best_medoids is None:
        kmedoids_ = KMedoids(n_clusters=t, metric='precomputed', method="pam")
    else:
        # ClasseDePissio is a class that inherits from KMedoids and overrides part of it to generate clusters with desired
        # medoids
        kmedoids_ = ClasseDePissio(n_clusters=t, init='random', metric='precomputed', method="pam",
                                   best_medoids=best_medoids)

    # Perform clustering
    kmedoids_.fit(distance_matrix)

    # Retrieve medoids
    medoids_idx = kmedoids_.medoid_indices_
    medoids = [sample[idx] for idx in medoids_idx]

    # Calculate clustering error and assign cluster membership for each point
    labels = kmedoids_.labels_
    error = 0
    for i in range(len(sample)):
        error += distance_matrix[i, medoids_idx[labels[i]]]

    # Retrieve points belonging to each cluster
    clusters = [[] for _ in range(t)]
    for i, label in enumerate(labels):
        clusters[label].append(sample[i])

    """
    In the global_search phase, I have a key that identifies each sample, whereas in the refinement (working on the
    whole dataset), I do not have this key. Therefore, I distinguish two types of returns, one with the key to
    distinguish the medoids for each sample, and the other (which will be used for refinement) without it.
    """
    if key is None:
        return {'medoids': medoids, 'clusters': clusters, 'error': error}
    else:
        return (key, {'medoids': medoids, 'clusters': clusters, 'error': error})

def export_to_mongo(data: list[dict]):
    # Connection
    client = pymongo.MongoClient("mongodb://localhost:27017/")

    # Database and collection selection.
    db = client["pamae"]
    collection = db["clustering_output"]

    """
    This is a custom JSON encoder class that converts numpy data types to Python data types.
    It is defined with a default method that checks the type of the object and returns a Python equivalent 
    if the object is of numpy data type. 
    If it is not, it calls the default method of the parent class.
    """
    class CustomEncoder(json.JSONEncoder):
        def default(self, obj):
            if isinstance(obj, np.integer):
                return int(obj)
            elif isinstance(obj, np.floating):
                return float(obj)
            elif isinstance(obj, np.ndarray):
                return obj.tolist()
            else:
                return super(CustomEncoder, self).default(obj)

    #This line uses the json.dumps() method to convert the data list of dictionaries to a JSON string
    # while using the CustomEncoder class for encoding.
    result_json = json.dumps(data, cls=CustomEncoder)
    #This line uses the json.loads() method to convert the JSON string result_json to a Python dictionary result_dict.
    result_dict = json.loads(result_json)

    print(result_dict)

    collection.insert_many(result_dict)


def parallel_seeding(samples: ps.RDD[Sample], t: int, sample_size: int) -> np.ndarray:
    """
    Performs Phase I of PAMAE algorithm on the collection of samples.

    :param samples: collection of objects Sample
    :param t: number of cluster (medoids)
    :param sample_size: sample size
    :return: 1D array of k elements containing the best medoids among the optimal medoids of each sample
    """

    # For each sample in the RDD, performs clustering and returns the results as a new RDD
    rdd_seeding = samples.map(lambda sample: clustering(sample.rows, t, None, sample.key))

    #TODO: testare clustering a 4 colonne
    # Initialize the errors array
    errors = np.empty([0, 3])

    # Collect the results
    result = rdd_seeding.collect()

    # Print the results
    for key, value in result:
        print(f"Sample {key}:")
        for i, cluster in enumerate(value['clusters']):
            print(f"Cluster {i}:")
            for point in cluster:
                print(point)
        print(f"Medoids:")
        for medoid in value['medoids']:
            print(medoid)


        errors = np.append(errors, np.array([medoid[0], medoid[1], value['error']]).reshape(1, -1), axis=0)
        print(f"Clustering error: {value['error']}")
        print()


    # Sort the errors in ascending order
    sorted_errors = errors[errors[:, 2].argsort()]
    print("SORTED ERRORS ", sorted_errors)

    # Print the set of best medoids
    print(f"Set of best medoids: {value['medoids']}")
    print(f"Minimum error: {sorted_errors[0, 2]}")

    print("---------------------------------------------------------")

    # Calculate the relative errors
    normalized_errors = []

    error_rel = sorted_errors[0, 2] / sample_size
    normalized_errors.append(error_rel)

    print("Normalized Error: ", normalized_errors)
    best_medoids = value['medoids']

    # Plot the results
    # plot the results for each sample
    for key, value in result:
        # create a new figure
        plt.figure()

        # iterate over each cluster
        for i, cluster in enumerate(value['clusters']):
            cluster = np.array(cluster)
            # plot the points in the cluster
            plt.scatter(cluster[:, 0], cluster[:, 1], label=f"Cluster {i}")
            # plot the medoid for the cluster
            medoid = np.array(value['medoids'][i])
            plt.scatter(medoid[0], medoid[1], marker='x', s=200, linewidths=3, color='r')

        # add a legend to the plot
        plt.legend()
        # set the title of the plot
        plt.title(f"Sample {key}")
        # show the plot
        plt.show()
    print("Best Medoids: ", best_medoids)
    return best_medoids


def parallel_refinement(best_medoids: np.ndarray, dataset: ps.RDD, t: int) -> list[dict]:
    """
    Phase 2 of the algorithm presented in the PAMAE paper
    :param best_medoids: collection of the (ids of the) best medoids found in phase 1
    :param dataset: full dataset
    :param t: number of clusters
    :return: list of dictionaries where each dictionary represents a single iteration of clustering.
            Each dictionary has three keys:
            - medoids: a list of medoids, where each medoid is represented by a list of values;
            - clusters: a list of clusters, where each cluster is represented by a list of points,
                        where each point is represented by a list of values;
            - error: a numerical value representing the clustering error calculated for the current iteration.
    """

    # Serializing the collect of dataset to join all arrays into a single row
    rdd_array = sc.parallelize(dataset.collect())

    # Getting a list of arrays from the RDD
    array_list = rdd_array.collect()

    # Creating a new RDD from the list of arrays
    rdd_tuple = sc.parallelize([(array_list,)])

    # Performing distributed clustering, where row[0] is the set of points of the entire dataset
    rdd_refinement = rdd_tuple.map(lambda row: clustering(row[0], t, best_medoids))

    # Collecting the results of the RDD into a list of dictionaries
    result = rdd_refinement.collect()

    # Printing and pretty-printing the result for debugging purposes
    print("RESULT: ")
    print(type(result))
    pp = pprint.PrettyPrinter(indent=2)
    pp.pprint(result)

    # Calculating the relative error (weighted by the number of points)
    dataset_size = dataset.count()
    total_error = result[-1]['error']
    normalized_error = (total_error / dataset_size)
    print("Normalized Error: ", normalized_error)

    """
    This section is responsible for plotting the results of the clustering. It loops through each dictionary in 
    the result list, which represents a single iteration of clustering. 
    For each dictionary, it creates a new figure and plots each cluster and its corresponding medoid.

    > for value in result:: loop through each dictionary in the result list
    > plt.figure(): create a new figure for each iteration
    > for i, cluster in enumerate(value['clusters']):: loop through each cluster in the 'clusters' key of the 
        current dictionary, using enumerate() to get both the index i and the cluster cluster
    > cluster = np.array(cluster): convert the cluster list into a NumPy array for easier manipulation
    > plt.scatter(cluster[:, 0], cluster[:, 1], label=f"Cluster {i}"): plot the points in the cluster on a scatter plot, 
        using the first column for the x-axis and the second column for the y-axis. 
        The label parameter is set to "Cluster i" to distinguish each cluster.
    > medoid = np.array(value['medoids'][i]): get the medoid corresponding to the current cluster and 
        convert it to a NumPy array
    > plt.scatter(medoid[0], medoid[1], marker='x', s=200, linewidths=3, color='r'): plot the medoid as 
        a red cross ('x') with a size of 200 and a linewidth of 3
    > plt.legend(): show the legend for the plot, with labels for each cluster
    > plt.title("Clusters on the full dataset"): set the title of the plot
    > plt.show(): display the plot on the screen
    """
    for value in result:
        plt.figure()
        for i, cluster in enumerate(value['clusters']):
            cluster = np.array(cluster)
            plt.scatter(cluster[:, 0], cluster[:, 1], label=f"Cluster {i}")
            medoid = np.array(value['medoids'][i])
            plt.scatter(medoid[0], medoid[1], marker='x', s=200, linewidths=3, color='r')
        plt.legend()
        plt.title("Clusters on the full dataset")
        plt.show()
    # Returning the list of dictionaries containing the results of each iteration of clustering
    return result


pamae(ds_import, 2, 500, 15)